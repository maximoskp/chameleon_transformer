{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c8a7f3-ddb5-484f-b3e1-fff04bde9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "# from transformer.models import DecoderOnlyModel\n",
    "from data_utils.Datasets import SerializedConcatDataset, PermSerializedConcatDataset, BinarySerializer\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoConfig, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4fbd6f-5bdf-408d-b6ed-ae338c867e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id='any_tonality'):\n",
    "    '''\n",
    "    model_id: 'any_tonality' or 'c_major'\n",
    "    '''\n",
    "    with open('serializer.pkl', 'rb') as inp:\n",
    "        binser = pickle.load(inp)\n",
    "    \n",
    "    # define model\n",
    "    vocab_size = binser.vocab_size\n",
    "    d_model = 256\n",
    "    if model_id == 'c_major':\n",
    "        num_heads = 2\n",
    "        num_layers = 2\n",
    "        max_seq_length = 3795\n",
    "    else:\n",
    "        num_heads = 4\n",
    "        num_layers = 4\n",
    "        max_seq_length = 1063\n",
    "    d_ff = 256\n",
    "    dropout = 0.3\n",
    "    \n",
    "    # dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dev = torch.device(\"cpu\")\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        vocab_size=vocab_size,\n",
    "        n_positions=max_seq_length,\n",
    "        n_layer=num_layers,\n",
    "        n_head=num_heads,\n",
    "        pad_token_id=binser.padding,\n",
    "        bos_token_id=binser.padding,\n",
    "        eos_token_id=binser.padding,\n",
    "        n_embd=d_ff\n",
    "    )\n",
    "    transformer = GPT2LMHeadModel(config).to(dev)\n",
    "    transformer = transformer.to(dev)\n",
    "\n",
    "    if model_id == 'c_major':\n",
    "        saved_model_path = '../saved_models/melboost_cmaj_GPT2_serialized/melboost_cmaj_GPT2_serialized.pt'\n",
    "    else:\n",
    "        saved_model_path = '../saved_models/melboost_GPT2_serialized/melboost_GPT2_serialized.pt'\n",
    "    transformer.load_state_dict(torch.load(saved_model_path), strict=False)\n",
    "    \n",
    "    transformer.eval()\n",
    "    return transformer\n",
    "# end load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bc1804f-0207-4df3-ae3e-23dbb6ca946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_melody_pcps_with_model_id(melody_pcps, model_id):\n",
    "    model = load_model(model_id)\n",
    "    binser2 = BinarySerializer()\n",
    "    # melody_pcps to serialized\n",
    "    x_mel, _ = binser2.sequence_serialization( melody_pcps, np.array([]) )\n",
    "    # x_mel has 'end harmonizing' at the end - remove it\n",
    "    x_mel = x_mel[:-1]\n",
    "    # run model\n",
    "    # dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dev = torch.device(\"cpu\")\n",
    "    inp = torch.from_numpy( np.expand_dims(x_mel, axis=0)).to(dev)\n",
    "    output = model(inp, attention_mask=inp != 0, output_attentions=True)\n",
    "    prediction = output.logits.argmax(dim=2, keepdim=True).squeeze()\n",
    "    z = prediction.cpu().numpy()\n",
    "    print('x_mel: ', x_mel)\n",
    "    print('z: ', z)\n",
    "    # back to binary\n",
    "    bin_all = binser2.indexes2binary( list(x_mel) + list(z) )\n",
    "    # make sure length of melody and chords are equal\n",
    "    c = bin_all['chords']\n",
    "    m = bin_all['melody']\n",
    "    if c.shape[1] > m.shape[1]:\n",
    "        c = c[:, :m.shame[1]]\n",
    "    elif c.shape[1] < m.shape[1]:\n",
    "        c = np.c_[c, np.zeros( (12, m.shape[1] - c.shape[1] ) )]\n",
    "    return c, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53670bdd-f03d-4631-a179-df9cb88eec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5328, 129, 12)\n",
      "(129, 12)\n",
      "x_mel:  [ 1  2  3 12  2 14  2  3 12  2 14  2 10 14  2 12  2 10 14  2 12  2  3 12\n",
      "  2 14  2  3 12  2 10 14  2  7  2  7  2  4  7  2  6  2  4  7  2  6  2  6\n",
      " 14  2  4  2  6 14  2  4  2  4  7  2  6  2  4  7  2  6 14  2 11  2 10  2\n",
      "  3 12  2 14  2  3 12  2 14  2 10 14  2 12  2 10 14  2 12  2  3 12  2 14\n",
      "  2  3 12  2 10 14  2 10  2 10  2 12  2 10  2  8  2  7  2 10  2  8  2  5\n",
      "  7  2  3 12  2 14  2  3 12  2  7 14  2  3  2  3  2 15]\n",
      "z:  [15 15 15 16 15 16 15 16 16 15 16 15 16 15 15 16 15 16 15 15 15 15 16 15\n",
      " 15 15 15 16 15 15 16 15 15 16 15 15 15 16 15 15 15 15 16 15 15 15 15 15\n",
      " 15 15 15 15 16 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 24 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 16 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      " 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 16]\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n",
      "ERROR-start_harmonizing: already harmonizing\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# load data\n",
    "npz_path = '../data/augmented_and_padded_data.npz'\n",
    "data = np.load(npz_path)\n",
    "melody_pcps = data['melody_pcps'].astype('float32')\n",
    "print(melody_pcps.shape)\n",
    "melody_pcp = melody_pcps[0]\n",
    "print(melody_pcp.shape)\n",
    "\n",
    "# binser2 = BinarySerializer(left_padding=False)\n",
    "# x, _ = binser2.sequence_serialization( melody_pcp, np.array([]) )\n",
    "# # the last element is 'st'\n",
    "# print(x.shape)\n",
    "# with np.printoptions(threshold=np.inf):\n",
    "#     print(x)\n",
    "# print(mask.shape)\n",
    "\n",
    "model_id = 'c_major'\n",
    "# model_id = 'any_tonality'\n",
    "\n",
    "c, m = harmonize_melody_pcps_with_model_id(melody_pcp, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1edac08a-b203-4623-8146-a857ce6438e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "output:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(threshold=np.inf):\n",
    "    print('input: ', m)\n",
    "    print('output: ', c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
